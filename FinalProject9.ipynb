{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction, Creation, and Normalization\n",
    "\n",
    "In the data exploration stage we have identified the relevant features. In this combined notebok we will first prepare the features, and then build and evaluate our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "import numpy as np \n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Dur: string (nullable = true)\n",
      " |-- Proto: string (nullable = true)\n",
      " |-- SrcAddr: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Dir: string (nullable = true)\n",
      " |-- DstAddr: string (nullable = true)\n",
      " |-- Dport: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- sTos: string (nullable = true)\n",
      " |-- dTos: string (nullable = true)\n",
      " |-- TotPkts: string (nullable = true)\n",
      " |-- TotBytes: string (nullable = true)\n",
      " |-- SrcBytes: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We start by loading the CSV file that we are going to use\n",
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-e2505fd8-30ac-494c-84fd-b6d402f6066e',\n",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'api_key': 'Vflbn-q3SEKstW6WU0EqiO02GjNQo6bIb2UFbwUQ6ov5'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_01e0daecbfe749718a79c33cdabd6510_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('Sample2Capture.csv', 'default-donotdelete-pr-fuwiqkd4yylvy3'))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first thing we do is add the label to the dataset, \n",
    "# which we know according to the IP address of the infected PC\n",
    "\n",
    "df.createOrReplaceTempView('df') \n",
    "infected_addr = \"147.32.84.165\"\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT Dur, Proto, Sport, Dir, Dport, State, sTos, dTos, TotPkts,TotBytes,SrcBytes, \n",
    "CASE WHEN SrcAddr = '{infected_addr}' THEN 1 ELSE 0 END AS Bot\n",
    "from df\n",
    "\n",
    "\"\"\"\n",
    "df_current = spark.sql(sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dur: float (nullable = true)\n",
      " |-- Proto: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Dir: string (nullable = true)\n",
      " |-- Dport: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- sTos: string (nullable = false)\n",
      " |-- dTos: string (nullable = false)\n",
      " |-- TotPkts: float (nullable = true)\n",
      " |-- TotBytes: float (nullable = true)\n",
      " |-- SrcBytes: float (nullable = true)\n",
      " |-- Bot: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we convert relevant datatypes from string to integers and fill in nulls in two columns\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df_current = df_current.fillna({'sTos':'-1','dTos':'-1'})\n",
    "df_current = df_current.withColumn(\"Dur\", df_current[\"Dur\"].cast(FloatType()))\n",
    "df_current = df_current.withColumn(\"TotPkts\", df_current[\"TotPkts\"].cast(FloatType()))\n",
    "df_current = df_current.withColumn(\"TotBytes\", df_current[\"TotBytes\"].cast(FloatType()))\n",
    "df_current = df_current.withColumn(\"SrcBytes\", df_current[\"SrcBytes\"].cast(FloatType()))\n",
    "df_current = df_current.fillna({'sTos':'-1','dTos':'-1'})\n",
    "df_current.createOrReplaceTempView('df_current')\n",
    "df_current.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we index all the relevant categorical features.\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "\n",
    "state_indexer = StringIndexer(inputCol=\"State\", outputCol=\"StateIndex\")\n",
    "proto_indexer = StringIndexer(inputCol=\"Proto\", outputCol=\"ProtoIndex\")\n",
    "dir_indexer   = StringIndexer(inputCol=\"Dir\", outputCol=\"DirIndex\")\n",
    "sTos_indexer   = StringIndexer(inputCol=\"sTos\", outputCol=\"sTosIndex\")\n",
    "dTos_indexer   = StringIndexer(inputCol=\"dTos\", outputCol=\"dTosIndex\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just add the data indexers to a pipeline and transform the data accordingly\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[state_indexer, proto_indexer, dir_indexer,sTos_indexer,dTos_indexer])\n",
    "df_indexed =  pipeline.fit(df_current).transform(df_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we create the vector assembler with all the relevant data and the normalizer\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"StateIndex\",\"ProtoIndex\",\"DirIndex\", \"Dur\",\"TotBytes\",\"SrcBytes\",\"sTosIndex\",\"dTosIndex\"],outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features are now  prepared. \n",
    "#### We are now going to create a balanced training data set and a test set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first remove unused cols\n",
    "columns_to_drop = ['Proto', 'Sport','Dir','Dport','State','sTos','dTos']\n",
    "df_indexed = df_indexed.drop(*columns_to_drop)\n",
    "\n",
    "# split and balance the data\n",
    "splits = df_indexed.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]\n",
    "\n",
    "#now balance the classes in the training data set\n",
    "bot_data  = df_train.filter(df_train.Bot == 1) #get all the bot data\n",
    "normal_data_sampled  = df_train.filter(df_train.Bot == 0).sample(False,.01) #get all the normal data and sample 5% of it\n",
    "df_train_balanced = bot_data.union(normal_data_sampled) #union the downsampled normal data and the bot data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data:372715\n",
      "Train Set:298137\n",
      "Test Set:74578\n",
      "Bot Req in Test:2143\n",
      "Normal Req Sampled:2903\n",
      "Total Balanced Train Set:5046\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Data:{df_indexed.count()}\")\n",
    "print(f\"Train Set:{df_train.count()}\")\n",
    "print(f\"Test Set:{df_test.count()}\")\n",
    "print(f\"Bot Req in Test:{bot_data.count()}\")\n",
    "print(f\"Normal Req Sampled:{normal_data_sampled.count()}\")\n",
    "print(f\"Total Balanced Train Set:{df_train_balanced.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, LinearSVC\n",
    "#classifier = LinearSVC(maxIter=10, regParam=0.1, featuresCol='features_norm', labelCol='Bot')\n",
    "classifier = GBTClassifier(labelCol=\"Bot\", featuresCol=\"features_norm\", maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler,normalizer,classifier])\n",
    "model = pipeline.fit(df_train_balanced)\n",
    "prediction = model.transform(df_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98546397285766"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction.show()\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\").setLabelCol(\"Bot\")\n",
    "evaluator.evaluate(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9812975061402408"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.transform(df_test)\n",
    "#prediction.show()\n",
    "evaluator.evaluate(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505 bot requests out of 548 were correctly identified (92.15328467153284 %)\n"
     ]
    }
   ],
   "source": [
    "false_negatives = prediction.select(\"Bot\", \"prediction\", \"probability\").filter(\"Bot == 1 and prediction=0.0\").count()\n",
    "true_positives  = prediction.select(\"Bot\", \"prediction\", \"probability\").filter(\"Bot == 1 and prediction=1.0\").count()\n",
    "print(f\"{true_positives} bot requests out of {true_positives + false_negatives} were correctly identified ({true_positives / (true_positives + false_negatives) * 100} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5341 normal requests out of 74030 were wrongly identified (7.2146427124138865 %)\n"
     ]
    }
   ],
   "source": [
    "false_positives = prediction.filter(\"Bot == 0 and prediction=1\").count()\n",
    "true_negatives  = prediction.filter(\"Bot == 0 and prediction=0\").count()\n",
    "print(f\"{false_positives} normal requests out of {false_positives + true_negatives} were wrongly identified ({false_positives / (false_positives + true_negatives) * 100} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To summarize: our model wrongly identified about  7% of the normal requests as infected by maleware while it correctly identified 92% of the malware infected requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The detection rate is quite satisfying, though its up to the stakeholders to decide whether a false positive rate of 7% is accepable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
