{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Detection of Bot/Malware Generated Network Traffic - Model on Spark \n",
    "\n",
    "Malware traffic is often hard to detect as it uses real users' PC or browsers in order to generate fraudulent activity and Spam. The purpose of this project is to build a simple supervised model that will be trained to detect malware based traffic in a network traffic log or capture. When the model flags an IP as generating malware based spam and fraudulent activity  it can be listed for quarantine or further analysis. \n",
    "\n",
    "\n",
    "The Dataset used here is part of a larger dataset (named CTU-13) which records 4 hours of network traffic in a computer network of a university department in the CTU University, Czech Republic. The researchers that created the dataset infected one of the computers in the network in a malware that generates ClickFraud and Spam activity. The traffic was recorded by a traffic analytics tool which captured malware-based activity generated by the infected PC in addition to normal traffic. Since the infected computer is known, the data is labeled and the purpose of the project is to present a supervised classification model.\n",
    "\n",
    "### Feature Extraction, Creation, and Normalization\n",
    "\n",
    "In the data exploration stage we have identified the relevant features. In this combined notebok we will first prepare the features, and then build and evaluate our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20190813233752-0000\n",
      "KERNEL_ID = 5427d62f-bacf-4735-bd71-f5715de2131e\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "import numpy as np \n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Dur: string (nullable = true)\n",
      " |-- Proto: string (nullable = true)\n",
      " |-- SrcAddr: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Dir: string (nullable = true)\n",
      " |-- DstAddr: string (nullable = true)\n",
      " |-- Dport: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- sTos: string (nullable = true)\n",
      " |-- dTos: string (nullable = true)\n",
      " |-- TotPkts: string (nullable = true)\n",
      " |-- TotBytes: string (nullable = true)\n",
      " |-- SrcBytes: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We start by loading the CSV file that we are going to use\n",
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-e2505fd8-30ac-494c-84fd-b6d402f6066e',\n",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'api_key': 'Vflbn-q3SEKstW6WU0EqiO02GjNQo6bIb2UFbwUQ6ov5'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_01e0daecbfe749718a79c33cdabd6510_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('Sample2Capture.csv', 'default-donotdelete-pr-fuwiqkd4yylvy3'))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the interesting features discovered in the previous stage and add the label to the dataset.\n",
    "# we know the label according to the IP address of the infected PC\n",
    "\n",
    "df.createOrReplaceTempView('df') \n",
    "\n",
    "infected_addr = \"147.32.84.165\"\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT Dur, Proto, Sport, Dir, Dport, State, sTos, dTos, TotPkts,TotBytes,SrcBytes, \n",
    "CASE WHEN SrcAddr = '{infected_addr}' THEN 1 ELSE 0 END AS Bot\n",
    "from df\n",
    "\n",
    "\"\"\"\n",
    "df_current = spark.sql(sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dur: float (nullable = true)\n",
      " |-- Proto: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Dir: string (nullable = true)\n",
      " |-- Dport: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- sTos: string (nullable = false)\n",
      " |-- dTos: string (nullable = false)\n",
      " |-- TotPkts: float (nullable = true)\n",
      " |-- TotBytes: float (nullable = true)\n",
      " |-- SrcBytes: float (nullable = true)\n",
      " |-- Bot: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert relevant datatypes from string to integers and fill in some nulls in two columns\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df_current = df_current.fillna({'sTos':'-1','dTos':'-1'})\n",
    "df_current = df_current.withColumn(\"Dur\", df_current[\"Dur\"].cast(FloatType()))\n",
    "df_current = df_current.withColumn(\"TotPkts\", df_current[\"TotPkts\"].cast(FloatType()))\n",
    "df_current = df_current.withColumn(\"TotBytes\", df_current[\"TotBytes\"].cast(FloatType()))\n",
    "df_current = df_current.withColumn(\"SrcBytes\", df_current[\"SrcBytes\"].cast(FloatType()))\n",
    "\n",
    "df_current.createOrReplaceTempView('df_current')\n",
    "\n",
    "df_current.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index all the relevant categorical features.\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "\n",
    "state_indexer = StringIndexer(inputCol=\"State\", outputCol=\"StateIndex\")\n",
    "proto_indexer = StringIndexer(inputCol=\"Proto\", outputCol=\"ProtoIndex\")\n",
    "dir_indexer   = StringIndexer(inputCol=\"Dir\", outputCol=\"DirIndex\")\n",
    "sTos_indexer   = StringIndexer(inputCol=\"sTos\", outputCol=\"sTosIndex\")\n",
    "dTos_indexer   = StringIndexer(inputCol=\"dTos\", outputCol=\"dTosIndex\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data indexers to a pipeline and transform the data accordingly\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[state_indexer, proto_indexer, dir_indexer,sTos_indexer,dTos_indexer])\n",
    "df_indexed =  pipeline.fit(df_current).transform(df_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vector assembler with all the relevant data and the normalizer\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"StateIndex\",\"ProtoIndex\",\"DirIndex\", \"Dur\",\"TotBytes\",\"SrcBytes\",\"sTosIndex\",\"dTosIndex\"],outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features are   prepared. Now we are  going to create a balanced training data set and a test data set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first remove unused cols\n",
    "columns_to_drop = ['Proto', 'Sport','Dir','Dport','State','sTos','dTos']\n",
    "df_indexed = df_indexed.drop(*columns_to_drop)\n",
    "\n",
    "# split and balance the data\n",
    "splits = df_indexed.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]\n",
    "\n",
    "#now balance the classes in the training data set\n",
    "bot_data  = df_train.filter(df_train.Bot == 1) #get all the bot data\n",
    "#get all the normal data and sample %1 to approximate the size of the bot traffic sample\n",
    "normal_data_sampled  = df_train.filter(df_train.Bot == 0).sample(False,.01) \n",
    "#union the downsampled normal data and the bot data\n",
    "df_train_balanced = bot_data.union(normal_data_sampled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data:372715\n",
      "Train Set:298473\n",
      "Test Set:74242\n",
      "Bot Req in Test:2175\n",
      "Normal Req Sampled:3004\n",
      "Total Balanced Train Set:5179\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Data:{df_indexed.count()}\")\n",
    "print(f\"Train Set:{df_train.count()}\")\n",
    "print(f\"Test Set:{df_test.count()}\")\n",
    "print(f\"Bot Req in Test:{bot_data.count()}\")\n",
    "print(f\"Normal Req Sampled:{normal_data_sampled.count()}\")\n",
    "print(f\"Total Balanced Train Set:{df_train_balanced.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and evaluate a model based on spark's gradient boosted trees algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, LinearSVC\n",
    "classifier = GBTClassifier(labelCol=\"Bot\", featuresCol=\"features_norm\", maxIter=10)\n",
    "#another option that didnt perform well is: LinearSVC(maxIter=10, regParam=0.1, featuresCol='features_norm', labelCol='Bot')\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler,normalizer,classifier])\n",
    "model = pipeline.fit(df_train_balanced)\n",
    "prediction = model.transform(df_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9865475151904741"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction.show()\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\").setLabelCol(\"Bot\")\n",
    "evaluator.evaluate(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841129484891367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate the model against the test data\n",
    "prediction = model.transform(df_test)\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488 bot requests out of 516 were correctly identified (94.57364341085271 %)\n",
      "5302 normal requests out of 73726 were wrongly identified (7.1914928247836585%)\n"
     ]
    }
   ],
   "source": [
    "#check the confusion matrix\n",
    "\n",
    "false_negatives = prediction.filter(\"Bot == 1 and prediction=0.0\").count()\n",
    "true_positives  = prediction.filter(\"Bot == 1 and prediction=1.0\").count()\n",
    "\n",
    "tp = true_positives\n",
    "total = true_positives + false_negatives\n",
    "pr = true_positives / (true_positives + false_negatives) * 100\n",
    "\n",
    "print(f\"{tp} bot requests out of {total} were correctly identified ({pr}%)\")\n",
    "\n",
    "false_positives = prediction.filter(\"Bot == 0 and prediction=1\").count()\n",
    "true_negatives  = prediction.filter(\"Bot == 0 and prediction=0\").count()\n",
    "\n",
    "fp = false_positives\n",
    "total = false_positives + true_negatives\n",
    "pr = false_positives / (false_positives + true_negatives) * 100\n",
    "\n",
    "print(f\"{fp} normal requests out of {total} were wrongly identified ({pr}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To summarize: our model wrongly identified about  7% of the normal requests as infected by maleware while it correctly identified 92% of the malware infected requests.  The detection rate is quite satisfying, though its up to the stakeholders to decide whether a false positive rate of 7% is accepable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
